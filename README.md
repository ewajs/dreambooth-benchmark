# Dreambooth Model Benchmarking

This is a set of scripts designed to benchmark (or at least produce the benchmark dataset) for a given set of trained models. The script will take in a list of model folders stored in the `models/` folder and add regular Stable Diffusion v2 (`stabilityai/stable-diffusion-2`). The script will then apply a set of prompts defined in the file `benchmark_prompts.json`. In my case, trained with Dreambooth using the [Diffusers Repo example](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) to learn to draw Galileo, my cat as a specific with the same set of initial conditions (this is to ensure all models test the exact same conditions and if a model is run again on the same it will produce the exact same image due to the model being deterministic. This allows us to test **only** the difference made by the Dreambooth training process). And store all the images within a folder structure that will generate 2 images for each combination of a random seed (as many as defined in the json file), a guidance scale (out of 3: `4`, `7.5` and `9`), a number of inference steps (out of 3: `50`, `100` and `150`).

This generates **A LOT** of images per model, these values can be changed to produce fewer/more/different combination of inference values to apply.

In my case, I've trained several models varying training parameters in order to find the optimal combination of values. I've different GPU RAM environments which also required different optimizations in order to run training (locally with a 3070 Ti 8 Gb VRAM, on the cloud with Google Collab using a 40 Gb VRAM GPU, additionally local training resource constraints required me to use fp16 precision on the models).

There is a local version and a colab version which are esentially the same, except that the colab version will require a Google Drive mounted and the paths specified by command line in order to be easily reused as opposed to using the repo's own directory which in the case of Google Colab would be ephemeral (would get destroyed alongside the colab session).

To save time (and potentially money) the script checks whether the current prompt + settings has already been generated previously by checking the image files that are stored with a naming convention and avoids doing inference if the file is already present.

The prompts are stored without the target Dreambooth learned instance (what is being learned) token and this is injected into them by the script, for regular stable diffusion a class (the category of what is being learned) token is injected instead, this means that for stable diffusion the prompt will be different than for trained models, but this is an unavoidable parameter that needs to be changed as regular stable diffusion won't know what has been learned by Dreambooth. Using a class prompt helps us see what would've been drawn and in many cases it is remarkably close, save for the subject. In some cases it even was closer (for not very good models).  