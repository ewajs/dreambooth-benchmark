# Dreambooth Model Benchmarking

This is a set of scripts designed to benchmark (or at least produce the benchmark dataset) for a given set of trained models. The script will take in a list of model folders stored in the `models/` folder and add regular Stable Diffusion v2 (`stabilityai/stable-diffusion-2`). The script will then apply a set of prompts defined in the file `benchmark_prompts.json` (in my case, trained with Dreambooth using the [Diffusers Repo example](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth)) with the same set of initial conditions (this is to ensure all models test the exact same conditions and if a model is run again on the same it will produce the exact same image due to the model being deterministic. This allows us to test **only** the difference made by the Dreambooth training process). And store all the images within a folder structure that will generate 2 images for each combination of a random seed (as many as defined in the json file), a guidance scale (out of 3: `4`, `7.5` and `9`), a number of inference steps (out of 3: `50`, `100` and `150`).

This generates **A LOT** of images per model, these values can be changed to produce fewer/more/different combination of inference values to apply.

In my case, I've trained several models varying training parameters in order to find the optimal combination of values. I've different GPU RAM environments which also required different optimizations in order to run training (locally with a 3070 Ti 8 Gb VRAM, on the cloud with Google Collab using a 40 Gb VRAM GPU, additionally local training resource constraints required me to use fp16 precision on the models).

There is a local version and a colab version which are esentially the same, except that the colab version will require a Google Drive mounted and the paths specified by command line in order to be easily reused as opposed to using the repo's own directory which in the case of Google Colab would be ephemeral (would get destroyed alongside the colab session).